{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the cell below to use CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ftfy regex tqdm\n",
    "!git clone https://github.com/openai/CLIP.git\n",
    "%cd CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP input : WC folder of sub-images\n",
    "## The code to produce the CLIP statistics (clip_stats.csv) is provided at the bottom of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import clip         # need to go to the CLIP repo (this is done in the previous cell)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('..')\n",
    "from img_bbox import quickview\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "prompt_list = [\"a picture of an orange in a tree\",\n",
    "               \"a picture of an unripe orange in a tree\",\n",
    "               \"a picture of a part of an orange in a tree\",\n",
    "               \"a picture of a part of an unripe orange in a tree\",\n",
    "               \"a picture of a lemon in a tree\",\n",
    "               \"a picture of an unripe lemon in a tree\",\n",
    "               # from here on, the labels are not related to fruits but background\n",
    "               \"a picture of leaves and tree branches without any ripe or unripe fruit\",\n",
    "               \"a picture of leaves and tree branches\",\n",
    "               \"a picture of parts of leaves and tree branches\",\n",
    "               \"a picture of parts of leaves and tree branches without any ripe or unripe fruit\",\n",
    "               \"a picture of a building\",\n",
    "               \"a picture of a part of a building\",\n",
    "               \"a picture of a part of the sky\",\n",
    "               \"a picture of dead leaves on the ground without any ripe or unripe fruit\",\n",
    "               \"a picture of dead leaves on the ground\",\n",
    "               \"a picture of the roots of a tree\",\n",
    "               \"a picture of the roots of a tree without any ripe or unripe fruit\"]\n",
    "\n",
    "def predict_class(img_name):\n",
    "    image = preprocess(Image.open(img_name)).unsqueeze(0).to(device)\n",
    "    text = clip.tokenize(prompt_list).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text)\n",
    "        \n",
    "        logits_per_image, logits_per_text = model(image, text)\n",
    "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "    return probs\n",
    "\n",
    "# ========================= the four next functions were used to control each subimage and its bboxes\n",
    " \n",
    "def get_lines(label_path:str):\n",
    "    lines=[]\n",
    "    with open(label_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    return lines\n",
    "\n",
    "def remove_bboxes(line_number:int, label_path:str):\n",
    "    lines = get_lines(label_path)\n",
    "    if len(lines) == 0:\n",
    "        print(\"No bounding boxes to remove\")\n",
    "        return\n",
    "    with open(label_path, 'w') as file:\n",
    "        for i, line in enumerate(lines):\n",
    "            if i != line_number:\n",
    "                file.write(line)\n",
    "\n",
    "def keep_only_bboxes(line_number:int, label_path:str):\n",
    "    lines = get_lines(label_path)\n",
    "    if len(lines) == 0:\n",
    "        print(\"No bounding boxes to keep\")\n",
    "        return\n",
    "    with open(label_path, 'w') as file:\n",
    "        for i, line in enumerate(lines):\n",
    "            if i == line_number:\n",
    "                file.write(line)\n",
    "\n",
    "def delete_img_label(path, filename):\n",
    "    os.remove(path + \"images/\" + filename)\n",
    "    os.remove(path + \"labels/\" + filename.replace(\"jpg\", \"txt\"))\n",
    "\n",
    "def remove_all_bboxes(label_path:str):\n",
    "    open(label_path, \"w\").close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example : check on afternoon sunny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_orange = \"../FINAL_SUBIMAGES/AS_CLIP_AFT_SUN/images/\"\n",
    "mean_list = []\n",
    "not_mean_list = []\n",
    "argmax_list = []\n",
    "not_argmax_list = []\n",
    "for filename in os.listdir(folder_orange):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n",
    "        pred_class = predict_class(folder_orange + filename)\n",
    "        if np.argmax(pred_class) < 6:\n",
    "            argmax_list.append(filename)\n",
    "        else:\n",
    "            not_argmax_list.append(filename)\n",
    "        if np.mean(pred_class[0][:6]) > np.mean(pred_class[0][6:]):\n",
    "            mean_list.append(filename)\n",
    "        else:\n",
    "            not_mean_list.append(filename)\n",
    "\n",
    "both_list = list(set(mean_list) & set(argmax_list)) # taking intersection to prevent duplicates (i.e. images that are both mean and argmax)\n",
    "not_both_list = list(set(not_mean_list) & set(not_argmax_list))\n",
    "\n",
    "print(f\"Argmax accuracy : {len(argmax_list) / (len(argmax_list) + len(not_argmax_list))}\")\n",
    "print(f\"Mean accuracy : {len(mean_list) / (len(mean_list) + len(not_mean_list))}\")\n",
    "print(f\"Both accuracy : {len(both_list) / (len(both_list) + len(not_both_list))} with {len(both_list) + len(not_both_list)} chosen images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete_img_label(\"data/PUT_TO_CLIP/\", tmp)\n",
    "# ind-=5\n",
    "ind-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_bboxes(0, tmp_text.replace(\"jpg\", \"txt\"))\n",
    "ind -=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_all_bboxes(tmp_text)\n",
    "ind-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = not_argmax_list[ind]\n",
    "print(ind)\n",
    "tmp_text = (\"data/CLIP_AFT_SUN/labels/\"+tmp).replace(\"jpg\", \"txt\")\n",
    "quickview(folder_orange+tmp, tmp_text)\n",
    "ind += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update of the subimages that were classified as argmax FN (mean already checked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_length = len(not_argmax_list)\n",
    "nb_deleted = 0\n",
    "for i, filename in enumerate(not_argmax_list):\n",
    "    tmp_text = (\"data/CLIP_AFT_SUN/labels/\"+filename).replace(\"jpg\", \"txt\")\n",
    "    if get_lines(tmp_text) == []:\n",
    "        delete_img_label(\"data/CLIP_AFT_SUN/\", filename)\n",
    "        nb_deleted += 1\n",
    "final_length = len(not_argmax_list) - nb_deleted\n",
    "print(f\"Deleted {nb_deleted} images, {initial_length} -> {final_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_orange = \"data/CLIP_AFT_SUN/images/\"\n",
    "mean_list = []\n",
    "not_mean_list = []\n",
    "argmax_list = []\n",
    "not_argmax_list = []\n",
    "for filename in os.listdir(folder_orange):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n",
    "        pred_class = predict_class(folder_orange + filename)\n",
    "        if np.argmax(pred_class) < 6:\n",
    "            argmax_list.append(filename)\n",
    "        else:\n",
    "            not_argmax_list.append(filename)\n",
    "        if np.mean(pred_class[0][:6]) > np.mean(pred_class[0][6:]):\n",
    "            mean_list.append(filename)\n",
    "        else:\n",
    "            not_mean_list.append(filename)\n",
    "\n",
    "both_list = list(set(mean_list) & set(argmax_list)) # taking intersection to prevent duplicates (i.e. images that are both mean and argmax)\n",
    "not_both_list = list(set(not_mean_list) & set(not_argmax_list))\n",
    "\n",
    "print(f\"Argmax accuracy : {len(argmax_list) / (len(argmax_list) + len(not_argmax_list))}\")\n",
    "print(f\"Mean accuracy : {len(mean_list) / (len(mean_list) + len(not_mean_list))}\")\n",
    "print(f\"Both accuracy : {len(both_list) / (len(both_list) + len(not_both_list))} with {len(both_list) + len(not_both_list)} chosen images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP stat plots\n",
    "One needs to run this from the CLIP folder (run the first two cells at the top of the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_stats_list = []\n",
    "ALL_WC_PATH = \"../FINAL_SUBIMAGES/\" # same structure as in the paper\n",
    "for folders in os.listdir(ALL_WC_PATH):\n",
    "    if folders != \".DS_Store\":\n",
    "        folder_path = ALL_WC_PATH + folders + \"/images/\"\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n",
    "                pred_class = predict_class(folder_path + filename)\n",
    "                stats = {\"WC\": filename[:2]}\n",
    "                for i, prob in enumerate(pred_class[0]):\n",
    "                    stats[f\"p{i}\"] = prob\n",
    "                all_stats_list.append(stats)\n",
    "all_stats = pd.DataFrame(all_stats_list)\n",
    "\n",
    "all_stats.to_csv(\"../clip_stats.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
